{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d1c604-3d7c-4148-9cc4-cff87d3613b0",
   "metadata": {},
   "source": [
    "## DATA 602 - Spring 2024\n",
    "### Homework Assignment 2\n",
    "Total points : 60<br>\n",
    " Please provide your solutions into the cells provided after question cells. You can create new cells as needed. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b597df-0dae-4628-aa47-e93eb61ab502",
   "metadata": {},
   "source": [
    "<b>Question 1</b> [<span style=\"color: red;\">20 points</span>]:<br>\n",
    "Consider the `spam_ham_dataset.csv` ([Original link](https://www.kaggle.com/datasets/venky73/spam-mails-dataset/data)). This is a dataset that can be useful for trying out different models for spam or ham (not spam) detection.\n",
    "Your job is to:\n",
    "1. Load the `csv` and tokenize the text contents (word tokenization) from the `text` column\n",
    "2. Remove stop words and punctuations\n",
    "3. Use either stemming or lemmatization to consolidate inflected words to their root words.\n",
    "4. Create `y` from the column for label (from the `label_num` column, 0 is ham and 1 is spam).\n",
    "5. Split the preprocessed emails (and labels) with `train_test_split` with a 80-20 split (Please remember to  use the `stratify=y`  parameter)<br>\n",
    "<b>Hint</b>: Suggest you to use NLTK for steps 2-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15dcc5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pooji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pooji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pooji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the Natural Language Toolkit library\n",
    "import nltk\n",
    "# downloading the WordNet corpus, which is a lexical database for the English language\n",
    "nltk.download('wordnet')\n",
    "# downloading the Punkt tokenizer models, which are used for tokenizing text into sentences\n",
    "nltk.download('punkt')\n",
    "# downloading a list of stopwords, which are common words that are often removed from text during text processing tasks\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4aa9ea6-c7a9-48e3-939d-35091b47dba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1        2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2        3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3        4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4        2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code goes here\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 1. Load the dataset\n",
    "df = pd.read_csv(\"spam_ham_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738f48c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (4136,) (4136,)\n",
      "Test set shape: (1035,) (1035,)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text contents\n",
    "df['tokens'] = df['text'].apply(lambda x: word_tokenize(str(x).lower()))\n",
    "\n",
    "# 2. Remove stopwords and punctuations\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word.isalnum() and word not in stop_words])\n",
    "\n",
    "# 3. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# 4. Create y from the label_num column\n",
    "y = df['label_num']\n",
    "\n",
    "# 5. Split the preprocessed emails and labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tokens'], y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Print shapes to verify the split\n",
    "print(\"Train set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299689ac-dd7e-4fc6-9170-1e1d91e53c39",
   "metadata": {},
   "source": [
    "<b>Question 2</b> [<span style=\"color: red;\">20 points</span>]:<br>\n",
    "Train  logistic regression models on the training set and print out the accuracy and F1 scores for the test set. Do this for :\n",
    "1. Feature vectors encoded using `CountVectorizer`.\n",
    "2. Feature vectors encoded using `TfidfVectorizer`.<br>\n",
    "\n",
    "<b>Hint </b>: You may want to use `Pipeline` for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9085add2-5aae-4f64-a64f-ce37c7a98796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for CountVectorizer:\n",
      "Accuracy: 0.961352657004831\n",
      "F1 Score: 0.9310344827586207\n",
      "\n",
      "Results for TfidfVectorizer:\n",
      "Accuracy: 0.9884057971014493\n",
      "F1 Score: 0.9801980198019802\n"
     ]
    }
   ],
   "source": [
    "#Your code goes here\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Logistic Regression model\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Convert token lists to strings\n",
    "X_train_str = X_train.apply(lambda x: ' '.join(x))\n",
    "X_test_str = X_test.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Pipeline with CountVectorizer\n",
    "count_vectorizer_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', logistic_regression)\n",
    "])\n",
    "\n",
    "# Pipeline with TfidfVectorizer\n",
    "tfidf_vectorizer_pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', logistic_regression)\n",
    "])\n",
    "\n",
    "# Fit CountVectorizer model\n",
    "count_vectorizer_pipeline.fit(X_train_str, y_train)\n",
    "\n",
    "# Fit TfidfVectorizer model\n",
    "tfidf_vectorizer_pipeline.fit(X_train_str, y_train)\n",
    "\n",
    "# Predictions\n",
    "count_predictions = count_vectorizer_pipeline.predict(X_test_str)\n",
    "tfidf_predictions = tfidf_vectorizer_pipeline.predict(X_test_str)\n",
    "\n",
    "# Calculate accuracy and F1 score for CountVectorizer\n",
    "count_accuracy = accuracy_score(y_test, count_predictions)\n",
    "count_f1_score = f1_score(y_test, count_predictions)\n",
    "\n",
    "# Calculate accuracy and F1 score for TfidfVectorizer\n",
    "tfidf_accuracy = accuracy_score(y_test, tfidf_predictions)\n",
    "tfidf_f1_score = f1_score(y_test, tfidf_predictions)\n",
    "\n",
    "# Print results\n",
    "print(\"Results for CountVectorizer:\")\n",
    "print(\"Accuracy:\", count_accuracy)\n",
    "print(\"F1 Score:\", count_f1_score)\n",
    "print()\n",
    "print(\"Results for TfidfVectorizer:\")\n",
    "print(\"Accuracy:\", tfidf_accuracy)\n",
    "print(\"F1 Score:\", tfidf_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58862e2-22b9-484e-bfb6-2cfed9565477",
   "metadata": {},
   "source": [
    "<b>Question 3</b> [<span style=\"color: red;\">20 points</span>]:<br>\n",
    "Train Multinomial Naive Bayes models on the training set and print out the accuracy and F1 scores for the test set. Do this for :\n",
    "1. Feature vectors encoded using `CountVectorizer`.\n",
    "2. Feature vectors encoded using `TfidfVectorizer`.<br>\n",
    "\n",
    "<b>Hint </b>: You may want to use `Pipeline` for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc6cf7cf-aa09-43a7-bc2b-300b6cde5753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for CountVectorizer:\n",
      "Accuracy: 0.9217391304347826\n",
      "F1 Score: 0.8451242829827915\n",
      "\n",
      "Results for TfidfVectorizer:\n",
      "Accuracy: 0.9178743961352657\n",
      "F1 Score: 0.836223506743738\n"
     ]
    }
   ],
   "source": [
    "#Your code goes here\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Multinomial Naive Bayes model\n",
    "multinomial_nb = MultinomialNB()\n",
    "\n",
    "# Convert token lists to strings\n",
    "X_train_str = X_train.apply(lambda x: ' '.join(x))\n",
    "X_test_str = X_test.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Pipeline with CountVectorizer\n",
    "count_vectorizer_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', multinomial_nb)\n",
    "])\n",
    "\n",
    "# Pipeline with TfidfVectorizer\n",
    "tfidf_vectorizer_pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', multinomial_nb)\n",
    "])\n",
    "\n",
    "# Fit CountVectorizer model\n",
    "count_vectorizer_pipeline.fit(X_train_str, y_train)\n",
    "\n",
    "# Fit TfidfVectorizer model\n",
    "tfidf_vectorizer_pipeline.fit(X_train_str, y_train)\n",
    "\n",
    "# Predictions\n",
    "count_predictions = count_vectorizer_pipeline.predict(X_test_str)\n",
    "tfidf_predictions = tfidf_vectorizer_pipeline.predict(X_test_str)\n",
    "\n",
    "# Calculate accuracy and F1 score for CountVectorizer\n",
    "count_accuracy = accuracy_score(y_test, count_predictions)\n",
    "count_f1_score = f1_score(y_test, count_predictions)\n",
    "\n",
    "# Calculate accuracy and F1 score for TfidfVectorizer\n",
    "tfidf_accuracy = accuracy_score(y_test, tfidf_predictions)\n",
    "tfidf_f1_score = f1_score(y_test, tfidf_predictions)\n",
    "\n",
    "# Print results\n",
    "print(\"Results for CountVectorizer:\")\n",
    "print(\"Accuracy:\", count_accuracy)\n",
    "print(\"F1 Score:\", count_f1_score)\n",
    "print()\n",
    "print(\"Results for TfidfVectorizer:\")\n",
    "print(\"Accuracy:\", tfidf_accuracy)\n",
    "print(\"F1 Score:\", tfidf_f1_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
