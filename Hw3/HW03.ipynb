{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0109966f-52c0-4ba3-91e2-4302747fd759",
   "metadata": {},
   "source": [
    "## DATA 602 - Spring 2024\n",
    "### Homework Assignment 3\n",
    "Total points : 60<br>\n",
    " Please provide your solutions into the cells provided after question cells. You can create new cells as needed. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be97481-09f8-43c7-9965-9d879075f6ae",
   "metadata": {},
   "source": [
    "<b>Question 1</b> [<span style=\"color: red;\">20 points</span>]:<br>\n",
    "Consider the `Fish.csv` dataset again\n",
    "Your job is to use `Species`and `Width` for predicting the `Weight` (target). You will need to one-hot encode `Species` in order to do this. Perform a 80-20 split, do the training with the help of linear regression and then print the RMSE and R2 scores on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3759c941-14c4-4975-b760-446ad8a2c926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 83.71011402365869\n",
      "R2 Score: 0.9507352480054513\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Fish.csv')\n",
    "\n",
    "# One-hot encode the 'Species' column\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "species_encoded = onehot_encoder.fit_transform(df[['Species']])\n",
    "species_encoded_df = pd.DataFrame(species_encoded, columns=[f'Species_{i}' for i in range(species_encoded.shape[1])])\n",
    "df_encoded = pd.concat([df, species_encoded_df], axis=1)\n",
    "df_encoded.drop(columns=['Species'], inplace=True)\n",
    "\n",
    "# Splitting the dataset into train and test sets (80-20 split)\n",
    "X = df_encoded.drop(columns=['Weight'])\n",
    "y = df_encoded['Weight']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculating RMSE and R2 score\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"R2 Score:\", r2)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='`sparse` was renamed to `sparse_output`')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e4e2f-9d57-4459-abfd-066e403ecd9c",
   "metadata": {},
   "source": [
    "<b>Question 2</b> [<span style=\"color: red;\">20 points</span>]:<br>\n",
    "Consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "536a15fd-d84f-4483-bba0-fcc1cecd18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "#Training set\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',categories = ['alt.atheism', 'comp.graphics'],remove=('headers', 'footers', 'quotes'))\n",
    "#Testing set\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',categories = ['alt.atheism', 'comp.graphics'],remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b7fe66-69d7-4a99-8281-6cac34c3e27d",
   "metadata": {},
   "source": [
    "Your task is to:\n",
    "1.  With the help of a Tfidf vectorizer, train logistic regression and knn models (for knn, use `n_neighbors=5` and algorithm set to `brute`) on `newsgroups_train`.\n",
    "2. Calculate and print the accuracy and f1 scores on the entire `newsgroups_test` set for both  models.\n",
    "3. Now, for both models, Using the `time.process_time()` function, calculate and print out the median time of performing `.predict` on the first 200 records in the test set for 100 runs. (Essentially do 100 iterations, in each iteration do `.predict` for `newsgroups_test.data[0:200]`)\n",
    "<br>\n",
    "\n",
    "<b>Note</b> : For better results, pass the stopwords from nltk into the tfidf vectorizer as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6cf2a8-8f37-4428-a56c-d650d3a57cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Accuracy: 0.9152542372881356\n",
      "Logistic Regression - F1 Score: 0.9146907157759091\n",
      "KNN - Accuracy: 0.4689265536723164\n",
      "KNN - F1 Score: 0.3190271688393855\n",
      "Logistic Regression - Median Prediction Time for first 200 records in test set (100 runs): 0.0 seconds\n",
      "KNN - Median Prediction Time for first 200 records in test set (100 runs): 1.03125 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the newsgroups dataset with specified categories and removing headers, footers, and quotes\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'comp.graphics'], remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=['alt.atheism', 'comp.graphics'], remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Define the TF-IDF vectorizer with stopwords\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = tfidf_vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# Train logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "logistic_model.fit(X_train_tfidf, newsgroups_train.target)\n",
    "\n",
    "# Train KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, algorithm='brute')\n",
    "knn_model.fit(X_train_tfidf, newsgroups_train.target)\n",
    "\n",
    "# Predictions on the entire test set\n",
    "logistic_predictions = logistic_model.predict(X_test_tfidf)\n",
    "knn_predictions = knn_model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy and F1 scores\n",
    "logistic_accuracy = accuracy_score(newsgroups_test.target, logistic_predictions)\n",
    "logistic_f1 = f1_score(newsgroups_test.target, logistic_predictions, average='weighted')\n",
    "\n",
    "knn_accuracy = accuracy_score(newsgroups_test.target, knn_predictions)\n",
    "knn_f1 = f1_score(newsgroups_test.target, knn_predictions, average='weighted')\n",
    "\n",
    "print(\"Logistic Regression - Accuracy:\", logistic_accuracy)\n",
    "print(\"Logistic Regression - F1 Score:\", logistic_f1)\n",
    "print(\"KNN - Accuracy:\", knn_accuracy)\n",
    "print(\"KNN - F1 Score:\", knn_f1)\n",
    "\n",
    "# Calculate median time for prediction on the first 200 records in the test set for 100 runs\n",
    "num_runs = 100\n",
    "logistic_prediction_times = []\n",
    "knn_prediction_times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.process_time()\n",
    "    logistic_model.predict(X_test_tfidf[:200])\n",
    "    end_time = time.process_time()\n",
    "    logistic_prediction_times.append(end_time - start_time)\n",
    "\n",
    "    start_time = time.process_time()\n",
    "    knn_model.predict(X_test_tfidf[:200])\n",
    "    end_time = time.process_time()\n",
    "    knn_prediction_times.append(end_time - start_time)\n",
    "\n",
    "median_time_logistic = np.median(logistic_prediction_times)\n",
    "median_time_knn = np.median(knn_prediction_times)\n",
    "\n",
    "print(\"Logistic Regression - Median Prediction Time for first 200 records in test set (100 runs):\", median_time_logistic, \"seconds\")\n",
    "print(\"KNN - Median Prediction Time for first 200 records in test set (100 runs):\", median_time_knn, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9dea04-303f-4bcf-a94f-9014c4dfedee",
   "metadata": {},
   "source": [
    "<b>Question 3</b> [<span style=\"color: red;\">20 points</span>]:<br>\n",
    "In the last question, you may have noticed that KNN performs far worse than logistic regression. Now, lets try a way different method (develop by UMBC professors!) for feature extraction called BWMD. Your tasks are :\n",
    "1. Install pyBWMD, you may first need to install `Cython`. Link to [pyBWMD](https://github.com/EdwardRaff/pyBWMD/tree/master)\n",
    "2. Study [this example](https://github.com/EdwardRaff/pyBWMD/blob/master/examples/20NewsGroups.ipynb) to see how to `vectorize` strings.\n",
    "3. Vectorize the training and test dataset (essentially we are encoding the text using `pyBWMD` instead of `TfidfVectorizer`. <b>Note:</b> The target can stay the same, don't vectorize them with `pyBWMD`.\n",
    "4. Now train a KNN model (5 neighbors) on the newly vectorized training set. Then compute the Accuracy and F1_score using the test set.\n",
    "<br>\n",
    "\n",
    "<b>Note</b>: Your results may slightly improve from the KNN for Tfidf but still not at the level of logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d8b4e1-b411-4cc9-9abf-4007afa95158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from pyBWMD.bwmd import vectorize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7684ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87d3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f626d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0054729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN with pyBWMD:\n",
      "Accuracy: 0.5451977401129944\n",
      "F1 Score: 0.4649829496327737\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load stopwords from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation and lowercase the text\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "# Load the dataset\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'comp.graphics'], remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=['alt.atheism', 'comp.graphics'], remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Preprocess text data\n",
    "X_train = [preprocess_text(text) for text in newsgroups_train.data]\n",
    "X_test = [preprocess_text(text) for text in newsgroups_test.data]\n",
    "\n",
    "# Assuming you have a function called 'vectorize' that transforms text data into vectors\n",
    "# Vectorize the datasets using pyBWMD\n",
    "X_train = vectorize(X_train)\n",
    "X_test = vectorize(X_test)\n",
    "\n",
    "# Target remains the same, no need to vectorize\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "# Train a KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate performance\n",
    "knn_pred = knn_model.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
    "knn_f1 = f1_score(y_test, knn_pred, average='weighted')\n",
    "print(\"KNN with pyBWMD:\")\n",
    "print(\"Accuracy:\", knn_accuracy)\n",
    "print(\"F1 Score:\", knn_f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
